\name{pdist}
\docType{methods}
\alias{pdist}
\alias{pdist,PSTf,PSTf-method}
%- Also NEED an '\alias' for EACH other topic documented here.
\title{
	Compute probabilistic divergence between two PST
}
%
\description{
Compute probabilistic divergence between two PST
}
\usage{
\S4method{pdist}{PSTf,PSTf}(x,y, method="cp", l, ns=5000, symetric=FALSE, output="all")
}
%- maybe also 'usage' for other objects documented here.
\arguments{
  \item{x}{
	A PST, that is an object of class \code{PSTf} as returned by the \code{\link{pstree}} or \code{\link{prune}} method.
}
  \item{y}{
	A PST, that is an object of class \code{PSTf} as returned by the \code{\link{pstree}} or \code{\link{prune}} method.
} 
  \item{method}{
	character. Method for computing distances. So far only one method is available.
}
 \item{l}{
	integer. Length of the sequence(s) to generate.
}
  \item{ns}{
	integer. Number sequences to generate.
}
  \item{symetric}{
	logical. If \code{TRUE}, the symetric version of the measure is returned, see details.
}
  \item{output}{
	character. See \code{value}.
}
}

\details{
The function computes a probabilistic divergence measure between PST \eqn{S_{A}} and \eqn{S_{B}} based on the measure originally proposed in \cite{Juang-1985} and \cite{Rabiner-1989} for the comparison of two (hidden) Markov models \eqn{S_{A}} and \eqn{S_{B}}
\deqn{
d(S_{A}, S_{B})=\frac{1}{\ell} [\log P^{S_{A}}(x)-\log P^{S_{B}}(x)]=\frac{1}{\ell}\log \frac{P^{S_{A}}(x)}{P^{S_{B}}(x)}
}
where \eqn{x=x_{1}, \ldots, x_{\ell}} is a sequence generated by model \eqn{S_{A}}, \eqn{P^{S_{A}}(x)} is the probability of $x$ given model \eqn{S_{A}} and \eqn{P^{S_{B}}(x)} is the probability of \eqn{x} given model \eqn{S_{B}}. The ratio between the two sequence likelihoods measures how many times the sequence \eqn{x} is more likely to have been generated by \eqn{S_{A}} than by \eqn{S_{2}}. 

As the number $n$ of generated sequences on which the measure is computed (or the length of a single sequence) approaches infinity, the expected value of \eqn{d(S_{A}, S_{B})} converges to \eqn{d_{KL}(S_{A}, S_{B})} \cite{Falkhausen-1995, He-2000}, the Kullback-Leibler (KL) divergence (also called information gain) used in information theory to measure the difference between two probability distributions.

The \code{pdist} function uses the following procedure to compute the divergence between two PST:
\itemize{
\item generate a ransom sample of \eqn{n} sequences (of length \eqn{\ell}) with model \eqn{S_{A}} using the \code{\link{generate}} method
\item predict the sequences with \eqn{S_{A}} and with \eqn{S_{B}}
\item compute 
\deqn{
d_{i}(S_{A}, S_{B})=\frac{1}{\ell} [\log P^{S_{A}}(x_{i})-\log P^{S_{B}}(x_{i}))], \; i=1, \ldots, n
}
\item the expected value 
\deqn{
E(d(S_{A}, S_{B}))
}
is the divergence between models \eqn{S_{A}} and \eqn{S_{B}} and is estimated as 
\deqn{
\hat{E}(d(S_{A}, S_{B}))=\frac{1}{n} \sum_{i=1}^{n} d_{i}(S_{A}, S_{B})
}
}

}
\value{
If \code{ouput="all"}, a vector containing the divergence value for each generated sequence, if \code{output="mean"}, the mean, i.e. expected value which is the divergence between models.
}
\references{
Juang, B. H. and Rabiner, L. R. (1985). A probabilistic distance measure for hidden Markov models. \emph{ATT Technical Journal} \bold{64}(2), 391-408.

Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. \emph{Proceedings of the IEEE} \bold{77}(2), 257-286.
}
\author{
Alexis gabadinho
}

\examples{
## Loading the 'SRH' data frame and 'SRH.seq' sequence object
data(SRH)

## computing age at wave 1 
age1999 <- 1999-SRH$birthy

## selecting aged <30 and aged >=60
g1.seq <- SRH.seq[which(age1999<30),]
g2.seq <- SRH.seq[which(age1999>=60),]

## fitting one model for each age group
C99 <- qchisq(0.99,6-1)/2
g1.pst <- pstree(g1.seq, with.missing=TRUE, nmin=2, ymin=0.001)
g1.pst <- prune(g1.pst, gain="G2", C=C99)

g2.pst <- pstree(g2.seq, with.missing=TRUE, nmin=2, ymin=0.001)
g2.pst <- prune(g2.pst, gain="G2", C=C99)

## computing the probabilistic divergence
## 500 sequences are generated and 500 distances
## are computed. The between models distance is the mean
## of the 500 distances
d1_2 <- pdist(g1.pst, g2.pst, l=11, ns=500)
mean(d1_2)
}

\keyword{models}

