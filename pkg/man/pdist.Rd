\name{pdist}
\docType{methods}
\alias{pdist}
\alias{pdist,PSTf,PSTf-method}
%- Also NEED an '\alias' for EACH other topic documented here.
\title{
	Compute probabilistic divergence between two PST
}
\description{
Compute probabilistic divergence between two PST
}
\usage{
\S4method{pdist}{PSTf,PSTf}(x,y, method="cp", l, ns=5000, symetric=FALSE, output="all", ...)
}
%- maybe also 'usage' for other objects documented here.
\arguments{
  \item{x}{
	A PST, that is an object of class \code{PSTf} as returned by the \code{\link{pstree}} or \code{\link{prune}} method.
}
  \item{y}{
	A PST, that is an object of class \code{PSTf} as returned by the \code{\link{pstree}} or \code{\link{prune}} method.
} 
  \item{method}{
	character. Method for computing distances. So far only one method is available.
}
 \item{l}{
	integer. Length of the sequence(s) to generate.
}
  \item{ns}{
	integer. Number sequences to generate.
}
  \item{symetric}{
	logical. If \code{TRUE}, the symetric version of the measure is returned, see details.
}
  \item{output}{
	character. See \code{value}.
}
  \item{\dots}{
%%     ~~Describe \code{\dots} here~~
}
}
\details{
The function computes a probabilistic divergence measure between PST \eqn{S_{A}} and \eqn{S_{B}} based on the measure originally proposed in \cite{Juang-1985} and \cite{Rabiner-1989} for the comparison of two (hidden) Markov models \eqn{S_{A}} and \eqn{S_{B}}
\deqn{
d(S_{A}, S_{B})=\frac{1}{\ell} [\log P^{S_{A}}(x)-\log P^{S_{B}}(x)]=\frac{1}{\ell}\log \frac{P^{S_{A}}(x)}{P^{S_{B}}(x)}
}
where \eqn{x=x_{1}, \ldots, x_{\ell}} is a sequence generated by model \eqn{S_{A}}, \eqn{P^{S_{A}}(x)} is the probability of $x$ given model \eqn{S_{A}} and \eqn{P^{S_{B}}(x)} is the probability of \eqn{x} given model \eqn{S_{B}}. The ratio between the two sequence likelihoods measures how many times the sequence \eqn{x} is more likely to have been generated by \eqn{S_{A}} than by \eqn{S_{2}}. 

As the number $n$ of generated sequences on which the measure is computed (or the length of a single sequence) approaches infinity, the expected value of \eqn{d(S_{A}, S_{B})} converges to \eqn{d_{KL}(S_{A}, S_{B})} \cite{Falkhausen-1995, He-2000}, the Kullback-Leibler (KL) divergence (also called information gain) used in information theory to measure the difference between two probability distributions.

The \code{pdist} function uses the following procedure to compute the divergence between two PST:
\itemize{
\item generate a ransom sample of \eqn{n} sequences (of length \eqn{\ell}) with model \eqn{S_{A}} using the \code{\link{generate}} method
\item predict the sequences with \eqn{S_{A}} and with \eqn{S_{B}}
\item compute 
\deqn{
d_{i}(S_{A}, S_{B})=\frac{1}{\ell} [\log P^{S_{A}}(x_{i})-\log P^{S_{B}}(x_{i}))], \; i=1, \ldots, n
}
\item the expected value 
\deqn{
E(d(S_{A}, S_{B}))
}
is the divergence between models \eqn{S_{A}} and \eqn{S_{B}} and is estimated as 
\deqn{
\hat{E}(d(S_{A}, S_{B}))=\frac{1}{n} \sum_{i=1}^{n} d_{i}(S_{A}, S_{B})
}
}

}
\value{
If \code{ouput="all"}, a vector containing the divergence value for each generated sequence, if \code{output="mean"}, the mean, i.e. expected value which is the divergence between models.
}
\references{
Juang, B. H. and Rabiner, L. R. (1985). A probabilistic distance measure for hidden Markov models. \emph{ATT Technical Journal} \bold{64}(2), 391-408.

Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. \emph{Proceedings of the IEEE} \bold{77}(2), 257-286.
}
\author{
Alexis gabadinho
}

%% ~Make other sections like Warning with \section{Warning }{....} ~

\seealso{
%% ~~objects to See Also as \code{\link{help}}, ~~~
}
\examples{
}
\keyword{models}

